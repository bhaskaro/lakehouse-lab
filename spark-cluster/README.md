# ğŸ“˜ Databricks Lakehouse Formats â€“ Spark Architecture Labs

This repository contains **hands-on Apache Spark labs** designed to explain **real-world Spark architectures** commonly used in **Databricks-style lakehouse platforms**.

The goal of this repo is to move beyond *hello-world Spark* and help you understand:

* How Spark clusters actually work
* The difference between **clusters**, **workers**, **executors**, and **tasks**
* Why Databricks uses multiple clusters
* How Spark scales in production
* How long-running, shuffle-heavy jobs behave

All labs are **Docker-based**, reproducible, and runnable on a single Linux machine.

---

## ğŸ¯ Who This Repo Is For

This repo is useful if you are:

* Learning Apache Spark beyond basics
* Preparing for Databricks / Spark interviews
* A cloud or data engineer wanting hands-on Spark internals
* Exploring lakehouse architectures (Delta / Iceberg / Parquet)
* Coming from a Databricks background and want to understand *whatâ€™s underneath*

---

## ğŸ“‚ Repository Structure

```
spark/
â”œâ”€â”€ multiple-clusters/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ apps/
â”‚
â””â”€â”€ single-cluster-multi-worker/
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ README.md
    â””â”€â”€ apps/
```

Each subdirectory is a **self-contained lab** with its own README and runnable examples.

---

## ğŸ§ª Available Labs

### 1ï¸âƒ£ Multiple Spark Clusters (Isolation Model)

**Path**

```
spark/multiple-clusters/
```

**What it demonstrates**

* Running multiple independent Spark clusters
* Each cluster has its own master, workers, and history server
* Jobs are isolated per cluster
* A single Spark job never spans multiple clusters

**Why this matters**
This mirrors:

* Databricks multi-workspace setups
* Team-level isolation
* Dev / Test / Prod separation

â¡ï¸ See `spark/multiple-clusters/README.md` for full details.

---

### 2ï¸âƒ£ Single Spark Cluster with Multiple Workers (Production Model)

**Path**

```
spark/single-cluster-multi-worker/
```

**What it demonstrates**

* One Spark cluster
* Many worker nodes
* A single job using all workers in parallel
* Executor and task distribution

**Why this matters**
This is the **most common real-world Spark architecture**, used by:

* Databricks job clusters
* Spark on YARN
* Spark on Kubernetes (conceptually)

â¡ï¸ See `spark/single-cluster-multi-worker/README.md` for full details.

---

## ğŸ§  Core Concepts Covered

This repo helps you clearly understand:

| Concept                  | Covered |
| ------------------------ | ------- |
| Cluster isolation        | âœ…       |
| Worker vs executor       | âœ…       |
| Task scheduling          | âœ…       |
| Shuffle-heavy workloads  | âœ…       |
| Long-running Spark jobs  | âœ…       |
| Spark UI analysis        | âœ…       |
| Failure & retry behavior | âœ…       |

---

## ğŸ§ª Types of Jobs Included

* Simple jobs (WordCount)
* CPU + memory intensive jobs
* Shuffle-heavy aggregations
* Disk IOâ€“intensive workloads
* Multi-minute runtime jobs (5â€“10+ minutes)

These are **intentionally realistic**, not toy examples.

---

## âš ï¸ Notes on Runtime Artifacts

The following are **intentionally excluded from Git**:

* Spark event logs
* Generated data
* Temporary shuffle directories

These are created at runtime and should not be version-controlled.

---

## ğŸš€ Recommended Learning Path

1. Start with **single-cluster-multi-worker**
2. Observe task distribution in Spark UI
3. Tune partitions and executor settings
4. Move to **multiple-clusters**
5. Understand isolation vs scalability trade-offs

This progression mirrors how Spark is learned in real teams.

---

## ğŸ”® Possible Extensions

This repo is designed to grow. Natural next steps include:

* Delta Lake examples
* Apache Iceberg examples
* Performance tuning guides
* Spark-on-Kubernetes labs
* Architecture diagrams
* Databricks mapping explanations

---

## ğŸ Summary

This repository is a **practical Spark architecture lab**, not just a code dump.

If you understand everything in this repo, you understand:

* How Spark really works
* Why Databricks is designed the way it is
* How to reason about Spark performance and scaling

---
