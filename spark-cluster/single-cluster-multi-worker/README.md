# ğŸ“˜ README â€” Single Spark Cluster with Multiple Workers (Production Model)

**Path**

```
spark/single-cluster-multi-worker/README.md
```

---

## ğŸ“˜ Spark Single Cluster with Multiple Workers

This lab demonstrates the **most common production Spark architecture**:

> **One Spark cluster with many worker nodes processing a single job in parallel.**

This is how Spark is used in:

* Databricks job clusters
* On-prem Spark standalone
* Spark on YARN
* Spark on Kubernetes (conceptually)

---

## ğŸ§  Key Concept

> **Parallelism in Spark comes from WORKERS and EXECUTORS, not from multiple clusters.**

A single job:

* Runs on one cluster
* Uses all workers in that cluster
* Scales horizontally with more workers

---

## ğŸ“‚ Directory Structure

```
single-cluster-multi-worker/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ wordcount.py
â”‚   â””â”€â”€ bigdata_job.py
â”œâ”€â”€ data/        # runtime only (ignored by git)
â””â”€â”€ eventlog/    # runtime only (ignored by git)
```

---

## â–¶ï¸ Start Cluster with 10 Workers

```bash
cd spark/single-cluster-multi-worker
docker compose up -d --scale spark-worker=10
```

Verify:

```bash
docker ps
```

---

## ğŸŒ Spark UIs

* **Spark Master UI** â†’ [http://localhost:8080](http://localhost:8080)
* **Spark History UI** â†’ [http://localhost:18080](http://localhost:18080)

You should see:

```
Alive Workers: 10
```

---

## â–¶ï¸ Run a Simple Job

```bash
docker compose exec spark-master \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/wordcount.py
```

---

## â–¶ï¸ Run a Big Data Job (5+ minutes)

```bash
docker compose exec spark-master \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.speculation=false \
  --conf spark.sql.shuffle.partitions=200 \
  /opt/spark-apps/bigdata_job.py
```

---

## ğŸ” What to Observe in Spark UI

### Executors Tab

* Executors spread across multiple workers
* Tasks executed in parallel
* CPU & memory utilization across workers

### Stages Tab

* Shuffle-heavy stages
* Long-running tasks
* Wide transformations (groupBy, sort)

---

## ğŸ§  Important Insight

> **Workers are fixed. Executors are dynamic.**

* You always have the same number of workers
* Spark creates executors based on workload
* More partitions â‡’ more tasks â‡’ more parallelism

---

## âœ… What This Lab Teaches

âœ” Real Spark scaling model
âœ” Worker vs executor behavior
âœ” Task scheduling
âœ” Shuffle cost & performance
âœ” Why single cluster scales better than multiple clusters

---

## ğŸ§ª Recommended Experiments

* Compare runtime with 2 vs 10 workers
* Change `spark.sql.shuffle.partitions`
* Adjust executor cores & memory
* Kill a worker and observe task retries
* Monitor shuffle read/write sizes

---

## ğŸ Summary

This lab demonstrates **how Spark actually scales in production**:

* One cluster
* Many workers
* One job using all available resources

This is the **core Spark execution model**.

---


